{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f080e369",
   "metadata": {},
   "source": [
    "오토인코더 만들기."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d99746e",
   "metadata": {},
   "source": [
    "1. Dataset\n",
    "    - CIFAR-10\n",
    "2. 이미지를 8x8 혹은 4x4 블록으로 쪼개서 DCT 변환 후, DCT Coeff를 Output으로 내는 AutoEncoder.\n",
    "3. 모델의 Output을 IDCT 해서 원래 이미지가 잘 나오는 지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4bb2924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 사용 가능한 GPU 개수: 1\n",
      "현재 선택된 GPU 번호: 0\n",
      "장치 이름: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # 물리적 순서대로 정렬\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"        # 나는 6번 GPU만 보이게 하겠다!\n",
    "\n",
    "import torch\n",
    "print(f\"현재 사용 가능한 GPU 개수: {torch.cuda.device_count()}\")\n",
    "print(f\"현재 선택된 GPU 번호: {torch.cuda.current_device()}\")\n",
    "print(f\"장치 이름: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c8cc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오토인코더\n"
     ]
    }
   ],
   "source": [
    "print(\"오토인코더\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f79aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torchvision import datasets, transforms\n",
    "# import numpy as np\n",
    "# from scipy.fftpack import dct, idct\n",
    "\n",
    "# # ---------------------------\n",
    "# # DCT / IDCT utilities\n",
    "# # ---------------------------\n",
    "\n",
    "# def dct2(block):\n",
    "#     return dct(dct(block.T, norm='ortho').T, norm='ortho')\n",
    "\n",
    "# def idct2(block):\n",
    "#     return idct(idct(block.T, norm='ortho').T, norm='ortho')\n",
    "\n",
    "# def blockify(img, block_size=8):\n",
    "#     blocks = []\n",
    "#     C, H, W = img.shape\n",
    "#     for c in range(C):\n",
    "#         for i in range(0, H, block_size):\n",
    "#             for j in range(0, W, block_size):\n",
    "#                 blocks.append(img[c, i:i+block_size, j:j+block_size])\n",
    "#     return blocks\n",
    "\n",
    "# def deblockify(blocks, img_shape, block_size=8):\n",
    "#     C, H, W = img_shape\n",
    "#     img = torch.zeros(img_shape)\n",
    "#     idx = 0\n",
    "#     for c in range(C):\n",
    "#         for i in range(0, H, block_size):\n",
    "#             for j in range(0, W, block_size):\n",
    "#                 img[c, i:i+block_size, j:j+block_size] = blocks[idx]\n",
    "#                 idx += 1\n",
    "#     return img\n",
    "\n",
    "# # ---------------------------\n",
    "# # AutoEncoder\n",
    "# # ---------------------------\n",
    "\n",
    "# class DCTAutoEncoder(nn.Module):\n",
    "#     def __init__(self, dim=64):\n",
    "#         super().__init__()\n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Linear(dim, 32),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(32, dim)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         z = self.encoder(x)\n",
    "#         out = self.decoder(z)\n",
    "#         return out\n",
    "\n",
    "# # ---------------------------\n",
    "# # Dataset\n",
    "# # ---------------------------\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "# dataset = datasets.CIFAR10(\n",
    "#     root=\"./data\",\n",
    "#     train=True,\n",
    "#     download=True,\n",
    "#     transform=transform\n",
    "# )\n",
    "\n",
    "# loader = torch.utils.data.DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=1,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# # ---------------------------\n",
    "# # Model / Optim\n",
    "# # ---------------------------\n",
    "\n",
    "# model = DCTAutoEncoder()\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# # ---------------------------\n",
    "# # Training (very small demo)\n",
    "# # ---------------------------\n",
    "\n",
    "# model.train()\n",
    "\n",
    "# for epoch in range(1):\n",
    "#     for img, _ in loader:\n",
    "#         img = img.squeeze(0)  # [3,32,32]\n",
    "\n",
    "#         blocks = blockify(img)\n",
    "#         dct_blocks = []\n",
    "\n",
    "#         for b in blocks:\n",
    "#             b_np = b.numpy()\n",
    "#             dct_b = dct2(b_np)\n",
    "#             dct_blocks.append(torch.tensor(dct_b).flatten())\n",
    "\n",
    "#         dct_blocks = torch.stack(dct_blocks)\n",
    "\n",
    "#         output = model(dct_blocks)\n",
    "#         loss = criterion(output, dct_blocks)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         print(\"loss:\", loss.item())\n",
    "#         break\n",
    "#     break\n",
    "\n",
    "# # ---------------------------\n",
    "# # Reconstruction check\n",
    "# # ---------------------------\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     img, _ = next(iter(loader))\n",
    "#     img = img.squeeze(0)\n",
    "\n",
    "#     blocks = blockify(img)\n",
    "#     recon_blocks = []\n",
    "\n",
    "#     for b in blocks:\n",
    "#         dct_b = dct2(b.numpy())\n",
    "#         dct_flat = torch.tensor(dct_b).flatten()\n",
    "#         out = model(dct_flat)\n",
    "#         out_block = out.view(8, 8).numpy()\n",
    "#         recon = idct2(out_block)\n",
    "#         recon_blocks.append(torch.tensor(recon))\n",
    "\n",
    "#     recon_img = deblockify(recon_blocks, img.shape)\n",
    "\n",
    "# print(\"Original min/max:\", img.min().item(), img.max().item())\n",
    "# print(\"Reconstructed min/max:\", recon_img.min().item(), recon_img.max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3b395f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 132\u001b[0m\n\u001b[1;32m    129\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    130\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 132\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# <- 이거 바꿔서, 평균내거나 오차율 작게 하기. \u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# 6. Visualization (fixed image) -> 이미지로 복원하는 과정.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# dct_ae_cifar10.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "from scipy.fftpack import dct, idct\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 1. DCT / IDCT utilities -> 데이터를 DCT 영역과 이미지 영역을 넘나들 수 있게 함.\n",
    "# -----------------------------\n",
    "\n",
    "def dct2(block):\n",
    "    # 2D DCT (orthonormal)\n",
    "    return dct(dct(block, axis=0, norm='ortho'), axis=1, norm='ortho')\n",
    "    # ㄴ DCT를 하면서, block은 8*8 이고, axis가 0이면 DCT의 C행렬이 왼쪽에 붙고, axis가 1이면 오른쪽(이건 C의 전치)에 붙는다. \n",
    "    # norm='ortho'는 C전치C가 항등행렬임을 정의한다. 정규화 과정이다.\n",
    "    # block 안에는 그냥 8*8 데이터 저장소일뿐, DCT 되기 전 데이터가 있을수도 있고, 그 후의 데이터가 있을 수도 있다. \n",
    "def idct2(block):\n",
    "    # 2D IDCT (orthonormal)\n",
    "    return idct(idct(block, axis=0, norm='ortho'), axis=1, norm='ortho')\n",
    "\n",
    "def image_to_dct_blocks(img, block_size=8): # 블록을 8*8로 쪼개기. \n",
    "    \"\"\"\n",
    "    img: (H, W) numpy array -> 높이, 너비. 32*32 흑백 이미지\n",
    "    return: (num_blocks, block_size*block_size) -> (16, 64)\n",
    "    \"\"\"\n",
    "    H, W = img.shape\n",
    "    blocks = [] # 8*8 블록 하나를 DCT 한 뒤, 1차원 64개로 플랫튼. \n",
    "    for i in range(0, H, block_size): # 0 8 16 24\n",
    "        for j in range(0, W, block_size):\n",
    "            block = img[i:i+block_size, j:j+block_size]\n",
    "            blocks.append(dct2(block).flatten()) # 블록 하나를 행이 큰단위, 열이 작은단위로 64개 원소 폄. 그걸 리스트의 한 원소로 추가\n",
    "    return np.stack(blocks) # (16, 64). 16개의 블록 펼쳐진게 쌓여있음. 리스트의 각 원소를 한 행으로 쌓음. \n",
    "                            # 사실 위랑 구조는 같은데, 연산 가능한 행렬로 바꾼 것임. 16행 64열 행렬. \n",
    "\n",
    "# 그냥 위의 역 과정임.\n",
    "def dct_blocks_to_image(blocks, H, W, block_size=8):\n",
    "    \"\"\"\n",
    "    blocks: (num_blocks, block_size*block_size)\n",
    "    return: reconstructed image (H, W)\n",
    "    \"\"\"\n",
    "    img = np.zeros((H, W))\n",
    "    idx = 0\n",
    "    for i in range(0, H, block_size):\n",
    "        for j in range(0, W, block_size):\n",
    "            block = blocks[idx].reshape(block_size, block_size)\n",
    "            img[i:i+block_size, j:j+block_size] = idct2(block)\n",
    "            idx += 1\n",
    "    return img\n",
    "\n",
    "# -----------------------------\n",
    "# 2. AutoEncoder definition\n",
    "# -----------------------------\n",
    "\n",
    "class DCTAutoEncoder(nn.Module): # 블록간 간섭 없고, 완전히 독립적으로 압축, 복원. 64개 들어가서 64개 나옴.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32), #정보가 강제로 사라졌다가 다시 추청한거라, 기존의 32차원의 정보를 완벽히 복원할 수 없음!!! *중요*\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        out = self.decoder(z)\n",
    "        return out\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Dataset (CIFAR-10) -> 컬러 이미지 50,000장을 흑백으로 바꾸고, 연산하기 쉽게 변환. \n",
    "# -----------------------------\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),  # 단순화를 위해 grayscale. 처리하면 RGB가 흑백사진으로 바뀜. 현재 코드는 컬러로 복원하지 않음.\n",
    "    transforms.ToTensor() # (1, 32, 32)로 변경되고, float32 [0, 1] 범위로 바뀜. 원래는 unit8 [0, 255] 임. \n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Training setup\n",
    "# -----------------------------\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DCTAutoEncoder().to(device)\n",
    "criterion = nn.MSELoss() # 기준은 Mean Squared Error\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3) # 인코더 디코더의 모든 파라미터를 최적화. \n",
    "# 옵티마이저는 다음 스텝에서 가중치를 고쳐줌. 모멘트를 고려하는건데, 이건 나중에 따로 학습하기. \n",
    "\n",
    "# -----------------------------\n",
    "# 5. Training loop\n",
    "# -----------------------------\n",
    "\n",
    "epochs = 5 # 전체 이미지를 5바뀌 학습.\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0 # loss 누적값인데, 학습이 진행되는지 확인용. \n",
    "    for img, _ in loader: # _는 라벨인데, AE에서는 사용 안 함. \n",
    "        img = img.squeeze().numpy()  # (32, 32) 원래는 (1, 1, 32, 32)였음. \n",
    "\n",
    "        dct_blocks = image_to_dct_blocks(img)  # (16, 64). 주파주 차원으로 옮김. \n",
    "        dct_blocks = torch.tensor(dct_blocks, dtype=torch.float32).to(device) # 텐서로 바꾸고, GPU에 올림. \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon = model(dct_blocks)\n",
    "        loss = criterion(recon, dct_blocks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() # <- 이거 바꿔서, 평균내거나 오차율 작게 하기. \n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] loss: {total_loss:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Visualization (fixed image) -> 이미지로 복원하는 과정.\n",
    "# -----------------------------\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad(): # 이 블록 안에서는 기울기 계산 안 함. \n",
    "    img, _ = dataset[0]\n",
    "    img = img.squeeze().numpy() # 순수 흑백이미지 생성. \n",
    "\n",
    "    dct_blocks = image_to_dct_blocks(img) # 플랫튼 한 (16, 64) 행렬 생성. \n",
    "    dct_blocks_t = torch.tensor(dct_blocks, dtype=torch.float32).to(device) # 텐서로 바꾸고, GPU에 올림. \n",
    "\n",
    "    recon_blocks = model(dct_blocks_t).cpu().numpy() # 인코딩 디코딩을 모두 통과함. 블록이 출력됨.\n",
    "    recon_img = dct_blocks_to_image(recon_blocks, 32, 32) # (16,64) 블록들을 다시 이미지로 복원. \n",
    "\n",
    "print(\"Original min/max:\", img.min(), img.max()) # 이론적으로 값의 범위는 0~1 사이여야 함. by transforms.ToTensor()\n",
    "print(\"Reconstructed min/max:\", recon_img.min(), recon_img.max()) # DCT계수는 실수 전 범위. \n",
    "\n",
    "plt.figure(figsize=(6,3)) # 원본과 이미지를 보여주는 그림. \n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Original\")\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2) # 복원이미지를 보여주는 그림. \n",
    "plt.title(\"Reconstructed\")\n",
    "plt.imshow(recon_img, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
